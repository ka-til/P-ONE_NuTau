{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using Liklihood Ratio to seperate between the Taus and the electrons\n",
    "'''\n",
    "\n",
    "from icecube import icetray, dataio, dataclasses, simclasses, clsim\n",
    "from icecube.icetray import I3Units, OMKey, I3Frame\n",
    "from icecube.dataclasses import ModuleKey\n",
    "from os.path import expandvars\n",
    "import numpy as np\n",
    "#import matplotlib.pylab as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats.distributions import chi2\n",
    "from likelihoodHelpers import log_likelihood_biGauss, log_likelihood_doublePeak, likelihood_ratio_doublePeak, likelihood_ratio_biGauss\n",
    "import scipy\n",
    "\n",
    "def likelihoodfit(frame, omgeo):\n",
    "    print('Likelihood fit function called')\n",
    "\n",
    "    tau_timeDiff = ([])\n",
    "    #tau_pVal = ([])\n",
    "    tau_LRR = ([])\n",
    "\n",
    "    e_timeDiff = ([])\n",
    "    #e_pVal = ([])\n",
    "    e_LRR = ([])\n",
    "\n",
    "\n",
    "    #file = dataio.I3File(str(infile))\n",
    "\n",
    "    mctree = frame[\"I3MCTree\"]\n",
    "    primary = mctree.primaries\n",
    "    lepton = dataclasses.I3MCTree.first_child(mctree, primary[0].id)\n",
    "\n",
    "    mcpeMap = frame['MCPESeriesMap']\n",
    "    noiseMap = frame['NoiseSeriesMap']\n",
    "\n",
    "    for omkey in mcpeMap.keys():\n",
    "        oKey = omgeo.get(omkey)\n",
    "\n",
    "        '''\n",
    "        Obtaining the timeList\n",
    "        '''\n",
    "        noise_mcpeList = noiseMap[omkey]\n",
    "        noise_timeList = np.array([mcpe.time for mcpe in noise_mcpeList])\n",
    "        mcpeList = mcpeMap[omkey]\n",
    "        timeList = np.array([mcpe.time for mcpe in mcpeList])\n",
    "        tot_timeList = np.append(timeList, noise_timeList)\n",
    "\n",
    "\n",
    "        '''\n",
    "        Removing DOMs with hits less than 250 Hits\n",
    "        '''\n",
    "        if len(tot_timeList) < 100:\n",
    "            #print('less than 250 hits - 1')\n",
    "            continue\n",
    "\n",
    "\n",
    "        '''\n",
    "        Calculating the mean and removing the tails\n",
    "        '''\n",
    "\n",
    "        mean = tot_timeList.mean()\n",
    "\n",
    "        select_time = tot_timeList[(tot_timeList > mean-50) & (tot_timeList < mean+50)]\n",
    "        new_mean = select_time.mean()\n",
    "\n",
    "        if len(select_time) < 50:\n",
    "            #print('less than 250 hits - 1')\n",
    "            continue\n",
    "\n",
    "        bins = np.arange(min(select_time), max(select_time), 1)\n",
    "        max_hitTimes = select_time[(select_time > (new_mean-40))&(select_time < (new_mean+40))]\n",
    "\n",
    "\n",
    "        #[using zscore to remove the effect of outliers from the analysis]\n",
    "\n",
    "        z = stats.zscore(max_hitTimes)\n",
    "\n",
    "        max_hitTimes = max_hitTimes[(z>-1.6)&(z < 1.2)]\n",
    "        new_mean = max_hitTimes.mean()\n",
    "\n",
    "        #Shifting mean to zero\n",
    "\n",
    "        timestamps = max_hitTimes - new_mean\n",
    "        final_mean = timestamps.mean()\n",
    "\n",
    "        if len(max_hitTimes) < 50:\n",
    "            #print('less than 250 hits - 2')\n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        Histogramming the data from simulation\n",
    "        '''\n",
    "\n",
    "        bins = np.arange(min(timestamps), max(timestamps), 1)\n",
    "        num, bin_edges = np.histogram(timestamps, bins=bins)\n",
    "        bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n",
    "\n",
    "        '''\n",
    "        Fitting bifurcated Gaussian and double bifurcated gaussian to\n",
    "        the mcpe hit time distributions for both tau and electron.\n",
    "        '''\n",
    "\n",
    "        #Single Peak\n",
    "\n",
    "        nll = lambda *args: log_likelihood_biGauss(*args)\n",
    "        initial_biGauss = np.array([final_mean, 50, 5, max(num)])\n",
    "        bnds_biGauss = ((min(bin_centers), max(bin_centers)), (0, 100), (0, 10), (0, 1e6))\n",
    "        soln_biGauss = minimize(log_likelihood_biGauss, initial_biGauss,\n",
    "                                args=(num, bin_centers),\n",
    "                                method='Powell',\n",
    "                                bounds = bnds_biGauss)\n",
    "\n",
    "        #Double Peak\n",
    "\n",
    "        nll = lambda *args: log_likelihood_doublePeak(*args)\n",
    "        initial_doublePeak = np.array([min(bin_centers)+10, 20, 1, max(num), final_mean, 20, 1, max(num)])\n",
    "        bnds_doublePeak = ((min(bin_centers), final_mean), (0, 100), (0, 10), (0, 1e6),\n",
    "                            (final_mean, max(bin_centers)), (0, 100), (0, 10), (0,1e6))\n",
    "        soln_doublePeak = minimize(log_likelihood_doublePeak, initial_doublePeak,\n",
    "                                    args=(num, bin_centers),\n",
    "                                    method='Powell',\n",
    "                                    bounds=bnds_doublePeak)\n",
    "\n",
    "        '''\n",
    "        Removing DOMs whose minimization is not successful\n",
    "        '''\n",
    "        if soln_biGauss.success == False or soln_doublePeak.success == False:\n",
    "            print('Removing DOMs whose minimization is not successful')\n",
    "            continue\n",
    "\n",
    "        '''\n",
    "        Calculating the Likelihood ratio for bifurcated gaussian\n",
    "        and double double bifurcated gaussian\n",
    "        '''\n",
    "        LR_biGauss = likelihood_ratio_biGauss(bin_centers[num>0], num[num>0], soln_biGauss.x[0],\n",
    "                                              soln_biGauss.x[1], soln_biGauss.x[2], soln_biGauss.x[3])\n",
    "        LR_doublePeak = likelihood_ratio_doublePeak(bin_centers[num>0], num[num>0], soln_doublePeak.x[0],\n",
    "                                                    soln_doublePeak.x[1],soln_doublePeak.x[2],\n",
    "                                                    soln_doublePeak.x[3], soln_doublePeak.x[4],\n",
    "                                                    soln_doublePeak.x[5], soln_doublePeak.x[6],\n",
    "                                                    soln_doublePeak.x[7])\n",
    "\n",
    "        '''\n",
    "        Calculating the p-value using the likelihood ratio\n",
    "        '''\n",
    "        #pVal_biGauss = chi2.sf(LR_biGauss, len(num) - 4)\n",
    "        #pVal_doublePeak = chi2.sf(LR_doublePeak, len(num) - 8)\n",
    "\n",
    "        '''\n",
    "        Are there any messed up p-values?\n",
    "\n",
    "        if pVal_biGauss != pVal_biGauss:\n",
    "            print('BiGauss is not well defined - ', str(lepton.type))\n",
    "            print('Minimisation - ', soln_biGauss.success)\n",
    "            print('Degrees of Freedom - ', len(num) - 4)\n",
    "            print('Log Likelihood - ', LR_biGauss)\n",
    "        if pVal_doublePeak != pVal_doublePeak:\n",
    "            print('double peak is not well defined - ', str(lepton.type))\n",
    "            print('Minimisation - ', soln_doublePeak.success)\n",
    "            print('Degrees of Freedom - ', len(num) - 8)\n",
    "            print('Log Likelihood - ', LR_doublePeak)\n",
    "\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        (x, y) values for the fit\n",
    "\n",
    "        x = np.linspace(min(bin_centers), max(bin_centers), 1000)\n",
    "        #x = np.linspace(0, max(bin_centers)+1e5, 1000)\n",
    "        y_biGauss = biGauss(x, soln_biGauss.x[0],\n",
    "                                soln_biGauss.x[1], soln_biGauss.x[2], soln_biGauss.x[3])\n",
    "        y_doublePeak = double_peak(x, soln_doublePeak.x[0], soln_doublePeak.x[1],\n",
    "                                            soln_doublePeak.x[2], soln_doublePeak.x[3], soln_doublePeak.x[4],\n",
    "                                            soln_doublePeak.x[5], soln_doublePeak.x[6], soln_doublePeak.x[7])\n",
    "\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        Calculating the time difference and p-value ratio of bigauss and double peak\n",
    "        '''\n",
    "        timeDifference_doublePeak = soln_doublePeak.x[4] - soln_doublePeak.x[0]\n",
    "        #pVal_ratio = pVal_doublePeak/pVal_biGauss\n",
    "        LRR = LR_doublePeak/LR_biGauss\n",
    "        twoDelLRR = 2*abs(LR_doublePeak - LR_biGauss)\n",
    "        #print(twoDelLRR)\n",
    "\n",
    "        '''\n",
    "        Note - Minimize for double peak can sometimes return a large\n",
    "        time difference despite the fact the data actually contains only one peak\n",
    "        Accounting for that below using Likelihood ratios. If the Likehihood ratios of\n",
    "        the single peak and double peak are similar the timedifference will be forced to\n",
    "        be zero\n",
    "        '''\n",
    "\n",
    "        if LRR >= 0.85 and LRR <=1.3:\n",
    "            print('Time Difference before -', timeDifference_doublePeak)\n",
    "            timeDifference_doublePeak = 0.\n",
    "            #if abs(timeDifference_doublePeak) < 100:\n",
    "                #plt.figure(figsize=(10,9))\n",
    "                #_ = plt.hist(timestamps, bins=bins, histtype='step')\n",
    "                #plt.title(str(lepton.type)+' timeDifference < 100 ' + str(abs(LR_doublePeak/LR_biGauss)))\n",
    "                #plt.plot(x, y_biGauss, '--', c = 'r')\n",
    "                #plt.plot(x, y_doublePeak, '--', c = 'k')\n",
    "                #plt.axvline(final_mean, c = 'b')\n",
    "\n",
    "\n",
    "\n",
    "        '''\n",
    "        Checking if there are any terrible fits\n",
    "        '''\n",
    "        amp1 = soln_doublePeak.x[3]\n",
    "        amp2 = soln_doublePeak.x[7]\n",
    "\n",
    "        if amp1/amp2 < 1/4 and amp1/amp2 > 4:\n",
    "            print('Removing terrible fits')\n",
    "\n",
    "        if amp1 < 0 or amp2 < 0:\n",
    "            print('Error in amp')\n",
    "\n",
    "        '''\n",
    "        Separating the time difference calculated above and appending the values\n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        Tau\n",
    "        '''\n",
    "        if lepton.type == 15 or lepton.type == -15:\n",
    "            tau_timeDiff = np.append(tau_timeDiff, timeDifference_doublePeak)\n",
    "            #tau_pVal = np.append(tau_pVal, pVal_ratio)\n",
    "            tau_LRR = np.append(tau_LRR, twoDelLRR)\n",
    "            #plt.title('E')\n",
    "\n",
    "        '''\n",
    "        Electron and Neutral Current\n",
    "        '''\n",
    "\n",
    "        if lepton.type == 11 or lepton.type == -11 or lepton.type == 12 or lepton.type == -12 or lepton.type == 16 or lepton.type == -16:\n",
    "\n",
    "            e_timeDiff = np.append(e_timeDiff, timeDifference_doublePeak)\n",
    "            #e_pVal = np.append(e_pVal, pVal_ratio)\n",
    "            e_LRR = np.append(e_LRR, twoDelLRR)\n",
    "\n",
    "\n",
    "    if len(tau_LRR) == 0 and len(e_LRR) == 0:\n",
    "        print('event rejected')\n",
    "        return 0, 0, 0, 0, 'rejected events'\n",
    "\n",
    "    if len(tau_LRR) > 0:\n",
    "        tau_max_val = max(tau_LRR)\n",
    "    else:\n",
    "        tau_max_val = -1e-9\n",
    "\n",
    "    if len(e_LRR) > 0:\n",
    "        e_max_val = max(e_LRR)\n",
    "    else:\n",
    "        e_max_val = -1e-9\n",
    "\n",
    "    if tau_max_val > e_max_val:\n",
    "        tau_LRR_2 = tau_LRR[tau_LRR != tau_max_val]\n",
    "        #tau_max_val2 = max(tau_LRR[tau_LRR != tau_max_val])\n",
    "        #tau_LRR_3 = tau_LRR_2[tau_LRR_2 != tau_max_val2]\n",
    "        #tau_max_val3 = max(tau_LRR_2[tau_LRR_2 != tau_max_val2])\n",
    "\n",
    "        tau_numMaxLLR = len(tau_LRR[tau_LRR == tau_max_val])\n",
    "        tau_timeDiff_LR = tau_timeDiff[tau_LRR == tau_max_val]\n",
    "        tau_max2DelLLR = ([])\n",
    "        for j in range(0, tau_numMaxLLR):\n",
    "            tau_max2DelLLR = np.append(tau_max2DelLLR, tau_max_val)\n",
    "            return tau_max2DelLLR, tau_timeDiff_LR, tau_LRR, tau_timeDiff, 'tau'\n",
    "\n",
    "    if e_max_val > tau_max_val:\n",
    "        e_LRR_2 = e_LRR[e_LRR != e_max_val]\n",
    "        #e_max_val2 = max(e_LRR[e_LRR != e_max_val])\n",
    "        #e_LRR_3 = e_LRR_2[e_LRR_2 != e_max_val2]\n",
    "        #e_max_val3 = max(e_LRR_2[e_LRR_2 != e_max_val2])\n",
    "\n",
    "        e_numMaxLLR = len(e_LRR[e_LRR == e_max_val])\n",
    "        e_timeDiff_LR = e_timeDiff[e_LRR == e_max_val]\n",
    "        e_max2DelLLR = ([])\n",
    "        for j in range(0, e_numMaxLLR):\n",
    "            e_max2DelLLR = np.append(e_max2DelLLR, e_max_val)\n",
    "            return e_max2DelLLR, e_timeDiff_LR, e_LRR, e_timeDiff, 'e'\n",
    "    else:\n",
    "        print('they are equal?!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
