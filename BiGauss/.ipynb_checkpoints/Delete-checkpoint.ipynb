{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icecube import icetray, dataio, dataclasses, simclasses, clsim\n",
    "from icecube.icetray import I3Units, OMKey, I3Frame\n",
    "from icecube.dataclasses import ModuleKey\n",
    "from os.path import expandvars\n",
    "import scipy.constants as spc\n",
    "import scipy as sc\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from scipy import stats\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats.distributions import chi2\n",
    "import scipy\n",
    "\n",
    "infile = '/data/p-one/akatil/step_4_medium_water/NuTau_NuE_20Events/step_4_'+str(713)+'_medium_water_custom_mDOM_noise.i3.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Functions used\n",
    "\n",
    "'''\n",
    "\n",
    "def gaussian(x, pos, wid, amp):\n",
    "    y = amp*np.exp(-4*np.log(2)*((x-pos)/(wid))**2)\n",
    "    return y\n",
    "\n",
    "def biGauss(x, pos, wid, r, amp):\n",
    "    mask = x < pos\n",
    "\n",
    "    y_all = ([])\n",
    "    for i in range(0, len(mask)):\n",
    "\n",
    "        if mask[i] == True:\n",
    "            m = 1\n",
    "            nm = 0\n",
    "        else:\n",
    "            m = 0\n",
    "            nm = 1\n",
    "        if r != 0:\n",
    "            y1 = gaussian(x[i],pos,r*wid/(r+1),amp)*m\n",
    "            y2 = gaussian(x[i],pos,wid/(r+1),amp)*nm\n",
    "            y = y1 + y2\n",
    "        else:\n",
    "            y = gaussian(x[i],pos,wid, amp)*nm\n",
    "\n",
    "        y_all = np.append(y_all, y)\n",
    "    return y_all\n",
    "\n",
    "def double_peak(x, pos1, wid1, r1, amp1, pos2, wid2, r2, amp2):\n",
    "    b1 = biGauss(x, pos1, wid1, r1, amp1)\n",
    "    b2 = biGauss(x, pos2, wid2, r2, amp2)\n",
    "    b = np.append(b1, b2)\n",
    "    return b1+b2\n",
    "\n",
    "def log_likelihood_biGauss(theta, n, x):\n",
    "    pos, wid, r, amp = theta\n",
    "    model = biGauss(x, pos, wid, r, amp)\n",
    "    L = model - (n*np.log(model))\n",
    "    #print('*****************One Peak***************')\n",
    "    #print(theta, np.sum(L))\n",
    "    return np.sum(L)\n",
    "\n",
    "def log_likelihood_doublePeak(theta, n, x):\n",
    "    pos1, wid1, r1, amp1, pos2, wid2, r2, amp2 = theta\n",
    "    model = double_peak(x, pos1, wid1, r1, amp1, pos2, wid2, r2, amp2)\n",
    "    L = model - (n*np.log(model))\n",
    "    #print('*****************Double Peak***************')\n",
    "    #print(theta, np.sum(L))\n",
    "    return np.sum(L)\n",
    "\n",
    "def likelihood_ratio_doublePeak(x, n, pos1, wid1, r1, amp1, pos2, wid2, r2, amp2):\n",
    "    model = double_peak(x, pos1, wid1, r1, amp1, pos2, wid2, r2, amp2)\n",
    "    val = model - n + (n*np.log(n/model))\n",
    "    #print('log - ', n/model, 'n - ', n)\n",
    "    return np.sum(val)\n",
    "\n",
    "def likelihood_ratio_biGauss(x, n, pos, wid, r, amp):\n",
    "    model = biGauss(x, pos, wid, r, amp)\n",
    "    val = model - n + (n*np.log(n/model))\n",
    "    #print('log - ', n/model, 'n - ', n)\n",
    "    return np.sum(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded geometry\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Loading the geometry\n",
    "\n",
    "'''\n",
    "\n",
    "gcd_file = dataio.I3File('/home/users/akatil/P-ONE/GCD_files/PONE_Phase1.i3.gz')\n",
    "cframe = gcd_file.pop_frame()\n",
    "geometry = cframe[\"I3Geometry\"]\n",
    "omgeo = geometry.omgeo\n",
    "print('loaded geometry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihoodfit():\n",
    "    print('Likelihood fit function called')\n",
    "    \n",
    "    tau_timeDiff = ([])\n",
    "    tau_pVal = ([])\n",
    "    tau_LRR = ([])\n",
    "    \n",
    "    e_timeDiff = ([])\n",
    "    e_pVal = ([])\n",
    "    e_LRR = ([])\n",
    "\n",
    "\n",
    "    file = dataio.I3File(str(infile))\n",
    "\n",
    "    f = 1\n",
    "    \n",
    "    for frame in file:\n",
    "        mctree = frame[\"I3MCTree\"]\n",
    "        primary = mctree.primaries\n",
    "        lepton = dataclasses.I3MCTree.first_child(mctree, primary[0].id)\n",
    "\n",
    "        mcpeMap = frame['MCPESeriesMap']\n",
    "        noiseMap = frame['NoiseSeriesMap'] \n",
    "\n",
    "        for omkey in mcpeMap.keys():\n",
    "            oKey = omgeo.get(omkey)\n",
    "\n",
    "            '''\n",
    "            Obtaining the timeList\n",
    "            '''\n",
    "            noise_mcpeList = noiseMap[omkey]\n",
    "            noise_timeList = np.array([mcpe.time for mcpe in noise_mcpeList])\n",
    "            mcpeList = mcpeMap[omkey]\n",
    "            timeList = np.array([mcpe.time for mcpe in mcpeList])\n",
    "            tot_timeList = np.append(timeList, noise_timeList)\n",
    "\n",
    "\n",
    "            '''\n",
    "            Removing DOMs with hits less than 250 Hits\n",
    "            '''\n",
    "            if len(tot_timeList) < 250:\n",
    "                continue\n",
    "\n",
    "\n",
    "            '''\n",
    "            Calculating the mean and removing the tails\n",
    "            '''\n",
    "\n",
    "            mean = tot_timeList.mean()\n",
    "\n",
    "            select_time = tot_timeList[(tot_timeList > mean-50) & (tot_timeList < mean+50)]\n",
    "            new_mean = select_time.mean()\n",
    "\n",
    "            bins = np.arange(min(select_time), max(select_time), 1)\n",
    "            max_hitTimes = select_time[(select_time > (new_mean-40))&(select_time < (new_mean+40))]\n",
    "\n",
    "\n",
    "            #[using zscore to remove the effect of outliers from the analysis]\n",
    "            \n",
    "            z = stats.zscore(max_hitTimes)\n",
    "            \n",
    "            max_hitTimes = max_hitTimes[(z>-1.6)&(z < 1.2)]\n",
    "            new_mean = max_hitTimes.mean()\n",
    "            \n",
    "            #Shifting mean to zero\n",
    "            \n",
    "            timestamps = max_hitTimes - new_mean\n",
    "            final_mean = timestamps.mean()\n",
    "            \n",
    "            if len(max_hitTimes) < 250:\n",
    "                continue\n",
    "            \n",
    "            '''\n",
    "            Histogramming the data from simulation\n",
    "            '''\n",
    "\n",
    "            bins = np.arange(min(timestamps), max(timestamps), 1)\n",
    "            num, bin_edges = np.histogram(timestamps, bins=bins)\n",
    "            bin_centers = (bin_edges[:-1]+bin_edges[1:])/2\n",
    "            \n",
    "            '''\n",
    "            Fitting bifurcated Gaussian and double bifurcated gaussian to \n",
    "            the mcpe hit time distributions for both tau and electron.\n",
    "            '''\n",
    "            \n",
    "            #Single Peak\n",
    "            \n",
    "            nll = lambda *args: log_likelihood_biGauss(*args)\n",
    "            initial_biGauss = np.array([final_mean, 50, 5, max(num)])\n",
    "            bnds_biGauss = ((min(bin_centers), max(bin_centers)), (0, 100), (0, 10), (0, 1e6))\n",
    "            soln_biGauss = minimize(log_likelihood_biGauss, initial_biGauss, \n",
    "                                    args=(num, bin_centers),\n",
    "                                    method='Powell', \n",
    "                                    bounds = bnds_biGauss)\n",
    "            \n",
    "            #Double Peak\n",
    "            \n",
    "            nll = lambda *args: log_likelihood_doublePeak(*args)\n",
    "            initial_doublePeak = np.array([min(bin_centers)+10, 20, 1, max(num), final_mean, 20, 1, max(num)])\n",
    "            bnds_doublePeak = ((min(bin_centers), final_mean), (0, 100), (0, 10), (0, 1e6),\n",
    "                               (final_mean, max(bin_centers)), (0, 100), (0, 10), (0,1e6))\n",
    "            soln_doublePeak = minimize(log_likelihood_doublePeak, initial_doublePeak, \n",
    "                                       args=(num, bin_centers),\n",
    "                                       method='Powell',\n",
    "                                       bounds=bnds_doublePeak)\n",
    "            \n",
    "            '''\n",
    "            Removing DOMs whose minimization is not successful\n",
    "            '''\n",
    "            if soln_biGauss.success == False or soln_doublePeak.success == False:\n",
    "                print('Removing DOMs whose minimization is not successful')\n",
    "                continue\n",
    "                \n",
    "            '''\n",
    "            Calculating the Likelihood ratio for bifurcated gaussian \n",
    "            and double double bifurcated gaussian\n",
    "            '''\n",
    "            LR_biGauss = likelihood_ratio_biGauss(bin_centers[num>0], num[num>0], soln_biGauss.x[0],\n",
    "                                             soln_biGauss.x[1], soln_biGauss.x[2], soln_biGauss.x[3])\n",
    "            LR_doublePeak = likelihood_ratio_doublePeak(bin_centers[num>0], num[num>0], soln_doublePeak.x[0], \n",
    "                                                        soln_doublePeak.x[1],soln_doublePeak.x[2], \n",
    "                                                        soln_doublePeak.x[3], soln_doublePeak.x[4],\n",
    "                                                        soln_doublePeak.x[5], soln_doublePeak.x[6], \n",
    "                                                        soln_doublePeak.x[7])\n",
    "            \n",
    "            '''\n",
    "            Calculating the p-value using the likelihood ratio\n",
    "            '''\n",
    "            pVal_biGauss = chi2.sf(LR_biGauss, len(num) - 4)\n",
    "            pVal_doublePeak = chi2.sf(LR_doublePeak, len(num) - 8)\n",
    "            \n",
    "            '''\n",
    "            Are there any messed up p-values?\n",
    "            '''\n",
    "            if pVal_biGauss != pVal_biGauss:\n",
    "                print('BiGauss is not well defined - ', str(lepton.type))\n",
    "                print('Minimisation - ', soln_biGauss.success)\n",
    "                print('Degrees of Freedom - ', len(num) - 4)\n",
    "                print('Log Likelihood - ', LR_biGauss)\n",
    "            if pVal_doublePeak != pVal_doublePeak:\n",
    "                print('double peak is not well defined - ', str(lepton.type))\n",
    "                print('Minimisation - ', soln_doublePeak.success)\n",
    "                print('Degrees of Freedom - ', len(num) - 8)\n",
    "                print('Log Likelihood - ', LR_doublePeak)\n",
    "                \n",
    "            '''\n",
    "            Calculating the time difference and p-value ratio of bigauss and double peak\n",
    "            '''\n",
    "            timeDifference_doublePeak = soln_doublePeak.x[4] - soln_doublePeak.x[0]\n",
    "            pVal_ratio = pVal_doublePeak/pVal_biGauss\n",
    "            LRR = LR_doublePeak/LR_biGauss\n",
    "            \n",
    "            '''\n",
    "            Note - Minimize for double peak can sometimes return a large \n",
    "            time difference despite the fact the data actually contains only one peak\n",
    "            Accounting for that below using Likelihood ratios. If the Likehihood ratios of\n",
    "            the single peak and double peak are similar the timedifference will be forced to \n",
    "            be zero\n",
    "            \n",
    "            '''\n",
    "            \n",
    "            if LRR >= 0.85 and LRR <=1.3:\n",
    "                print('Time Difference before -', timeDifference_doublePeak)\n",
    "                timeDifference_doublePeak = 0.\n",
    "                \n",
    "            '''\n",
    "            Checking if there are any terrible fits\n",
    "            '''\n",
    "            \n",
    "            if amp1/amp2 < 1/4 and amp1/amp2 > 4:\n",
    "                print('Removing terrible fits')\n",
    "                \n",
    "            if amp1 < 0 or amp2 < 0:\n",
    "                print('Error in amp')\n",
    "            \n",
    "            '''\n",
    "            Separating the time difference calculated above and appending the values\n",
    "            '''\n",
    "\n",
    "            '''\n",
    "            Tau\n",
    "            '''\n",
    "            if lepton.type == 15 or lepton.type == -15:\n",
    "                tau_timeDiff = np.append(tau_timeDiff, timeDifference_doublePeak)\n",
    "                tau_pVal = np.append(tau_pVal, pVal_ratio)\n",
    "                tau_LRR = np.append(tau_LRR, LRR)\n",
    "                #plt.title('E')\n",
    "\n",
    "            '''\n",
    "            Electron and Neutral Current\n",
    "            '''\n",
    "\n",
    "            if lepton.type == 11 or lepton.type == -11 or lepton.type == 12 or lepton.type == -12 or lepton.type == 16 or lepton.type == -16:\n",
    "\n",
    "                e_timeDiff = np.append(e_timeDiff, timeDifference_doublePeak)\n",
    "                e_pVal = np.append(e_pVal, pVal_ratio)\n",
    "                e_LRR = np.append(e_LRR, LRR)\n",
    "\n",
    "\n",
    "        f = f+1\n",
    "            \n",
    "                \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = dataio.I3File(str(infile))\n",
    "\n",
    "for frame in file:\n",
    "    print('Starting')\n",
    "    numHitsinDOM = ([])\n",
    "    numHits = ([])\n",
    "    mctree = frame[\"I3MCTree\"]\n",
    "    primary = mctree.primaries\n",
    "    lepton = dataclasses.I3MCTree.first_child(mctree, primary[0].id)\n",
    "\n",
    "    mcpeMap = frame['MCPESeriesMap']\n",
    "    noiseMap = frame['NoiseSeriesMap']\n",
    "\n",
    "    #looping through doms that have physics hits\n",
    "    for omkey in mcpeMap.keys():\n",
    "        oKey = omgeo.get(omkey)\n",
    "\n",
    "        '''\n",
    "        Obtaining the timeList\n",
    "        '''\n",
    "        noise_mcpeList = noiseMap[omkey]\n",
    "        noise_timeList = np.array([mcpe.time for mcpe in noise_mcpeList])\n",
    "        mcpeList = mcpeMap[omkey]\n",
    "        timeList = np.array([mcpe.time for mcpe in mcpeList])\n",
    "        tot_timeList = np.append(timeList, noise_timeList)\n",
    "        numHitsinDOM = np.append(tot_timeList, noise_timeList)\n",
    "\n",
    "        #print(numHitsinDOM)\n",
    "\n",
    "    numHits = np.append(numHits, sum(numHitsinDOM))\n",
    "    log_numHits = np.log10(numHits[numHits > 0])\n",
    "    print(log_numHits)\n",
    "    if log_numHits > 5.5 and log_numHits <= 6.0:\n",
    "        likelihoodfit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
